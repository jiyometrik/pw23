{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question 3: Logistic Regression\n",
    "\n",
    "Here, we build a simple logistic regression model to classify hotel reviews\n",
    "into two basic categories: those with negative and those with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"talk\", style=\"white\")  # , font=\"serif\")\n",
    "\n",
    "\n",
    "# natural language processing\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# machine learning imports\n",
    "from funcsigs import signature\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "DATADIR = os.path.join(\n",
    "    os.path.abspath(os.path.dirname(\"\")), \"../data\"\n",
    ")\n",
    "DF = pd.read_csv(\n",
    "    os.path.join(DATADIR, \"combined_sentiments.csv\"),\n",
    "    header=0,\n",
    "    sep=\",\",\n",
    "    on_bad_lines=\"skip\",\n",
    ")\n",
    "STOP = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "# lemmatise\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"lemmatises words by classifying them into their\n",
    "    respective parts of speech\"\"\"\n",
    "    if pos_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "\n",
    "def check_digits(text):\n",
    "    \"\"\"is there a digit in the text?\"\"\"\n",
    "    return any(i.isdigit() for i in text)\n",
    "\n",
    "\n",
    "def clean_review(review):\n",
    "    \"\"\"tokenise and clean up punctuation\"\"\"\n",
    "    review = str(review).lower()\n",
    "    review = [\n",
    "        word.strip(string.punctuation)\n",
    "        for word in review.split(\" \")\n",
    "    ]  # remove punctuation\n",
    "    review = [\n",
    "        word for word in review if not check_digits(word)\n",
    "    ]  # remove digits\n",
    "\n",
    "    # remove stop words\n",
    "    review = [\n",
    "        token for token in review if token not in STOP\n",
    "    ]\n",
    "\n",
    "    # remove empty tokens\n",
    "    review = [token for token in review if len(token) > 0]\n",
    "\n",
    "    # tag each token with its part of speech (pos)\n",
    "    pos_tags = pos_tag(review)\n",
    "    review = [\n",
    "        WordNetLemmatizer().lemmatize(\n",
    "            tag[0], get_wordnet_pos(tag[1])\n",
    "        )\n",
    "        for tag in pos_tags\n",
    "    ]\n",
    "\n",
    "    # remove words with only one letter\n",
    "    review = [token for token in review if len(token) > 1]\n",
    "    review = \" \".join(review)\n",
    "    return review\n",
    "\n",
    "\n",
    "# generate a cleaned, tokenised and lemmatised version of the reviews\n",
    "DF[\"reviews.clean\"] = DF[\"reviews.text\"].apply(clean_review)\n",
    "REVIEWS_CLEAN = DF[\"reviews.clean\"]\n",
    "\n",
    "# get a list of all the reviews, and extract all the tokens as one big list\n",
    "REVIEWS_ALL = DF[\"reviews.clean\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Let's get the term frequency-inverse document frequencies for each token\n",
    "in each review.\n",
    "\n",
    "The **raw term frequency** for a token in a document (a review) is $\\text{tf}\\left(t, d\\right)$,\n",
    "where $t$ is a term (a token) and $d$ is a document.\n",
    "The number of documents where where term $t$ appears is $\\text{df}\\left(t, d\\right)$,\n",
    "and the inverse document frequency $\\text{idf}\\left(t, d\\right) = \\log \\frac{n_d}{1 + \\text{df}\\left(d, t\\right)}$.\n",
    "\n",
    "The TF-IDF is thus $\\text{tfidf}\\left(t, d\\right) = \\text{tf}\\left(t, d\\right)\\text{idf}\\left(t, d\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature vector that counts the number of times each word appears in the review\n",
    "COUNT_VECTORISER = CountVectorizer()\n",
    "WORDBAG = COUNT_VECTORISER.fit_transform(REVIEWS_ALL)\n",
    "\n",
    "COUNT_VECTORISER.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tfidf transformer instance...\n",
    "TFIDF_TRANSFORM = TfidfTransformer(\n",
    "    use_idf=True,\n",
    "    norm=\"l2\",\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "\n",
    "# ...and feed it the data\n",
    "TFIDF_TRANSFORM.fit_transform(WORDBAG).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing to train\n",
    "\n",
    "Let's split our datasets, with the independent variable being\n",
    "the token counts in each review and the dependent variable being the polarity\n",
    "of the review (positive or negative).\n",
    "\n",
    "We'll also create a **parameter grid** for the model to select from, to determine\n",
    "the best parameters to train the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test and validation sets\n",
    "X = REVIEWS_CLEAN\n",
    "y = DF[\"sent.polarity\"]\n",
    "\n",
    "X_t, X_test, y_t, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_t, y_t, test_size=0.25, random_state=1, stratify=y_t\n",
    ")\n",
    "\n",
    "# create a parameter grid for the model to pick the best params\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"vect__ngram_range\": [(1, 1)],\n",
    "        \"vect__stop_words\": [STOP, None],\n",
    "        \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "        \"clf__C\": [1.0, 10.0, 100.0],\n",
    "    },\n",
    "    {\n",
    "        \"vect__ngram_range\": [(1, 1)],\n",
    "        \"vect__stop_words\": [STOP, None],\n",
    "        \"vect__use_idf\": [False],\n",
    "        \"vect__norm\": [None],\n",
    "        \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "        \"clf__C\": [1.0, 10.0, 100.0],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_VECTORISER = TfidfVectorizer(\n",
    "    strip_accents=None, lowercase=False, preprocessor=None\n",
    ")\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TFIDF_VECTORISER),\n",
    "        (\"clf\", LogisticRegression(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "    pipeline,\n",
    "    PARAM_GRID,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Accuracy\n",
    "\n",
    "Next, we can calculate the best and in-test accuracies. We can also plot\n",
    "graphs, the precision-recall and receiver operating characteristic\n",
    "graph, to evaluate the model's precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best accuracy: {gridsearch.best_score_:.5f}\")\n",
    "\n",
    "clf = gridsearch.best_estimator_\n",
    "print(f\"accuracy in test: {clf.score(X_test, y_test):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "We can ask the model to create a list of predictions for us from\n",
    "values in the dataset. We can then calculate metrics such as the\n",
    "false positive and true positive rate, which will aid us in plotting\n",
    "the receiver operating characteristic graph later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "PREDS = clf.predict(X_val)\n",
    "\n",
    "\n",
    "ACTUALS = y_val.to_numpy()\n",
    "\n",
    "# standardise to binary classifications\n",
    "\n",
    "\n",
    "ACTUALS[ACTUALS == -1] = 0\n",
    "PREDS[PREDS == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic\n",
    "\n",
    "The receiver operating characteristic curve plots the rate of true positive predictions made\n",
    "against the rate of false positive predictions. The dotted line shows\n",
    "a scenario where the two are equal, and the plotted curve shows how well\n",
    "the model fares against it. The higher the curve lies over the dotted line,\n",
    "the higher the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTSDIR = os.path.join(\n",
    "    os.path.abspath(os.path.dirname(\"\")),\n",
    "    \"../results/\",\n",
    ")\n",
    "\n",
    "\n",
    "FALSE_POS_RATE, TRUE_POS_RATE, thresholds = roc_curve(\n",
    "    ACTUALS, PREDS, pos_label=1\n",
    ")\n",
    "roc_auc = auc(FALSE_POS_RATE, TRUE_POS_RATE)\n",
    "\n",
    "plt.plot(\n",
    "    FALSE_POS_RATE,\n",
    "    TRUE_POS_RATE,\n",
    "    label=f\"ROC (Area: {roc_auc:.3f})\",\n",
    ")\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    linestyle=\"--\",\n",
    "    label=\"Random classifier\",\n",
    ")\n",
    "\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC (logistic regression)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTSDIR, \"logistic_roc.png\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-recall\n",
    "\n",
    "> Precision-recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n",
    "> The precision-recall curve shows the tradeoff between precision and recall for different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_PRECISION = average_precision_score(\n",
    "    ACTUALS, PREDS, pos_label=1\n",
    ")\n",
    "PRECISION, RECALL, _ = precision_recall_curve(\n",
    "    ACTUALS, PREDS\n",
    ")\n",
    "step_kwargs = (\n",
    "    {\"step\": \"post\"}\n",
    "    if \"step\" in signature(plt.fill_between).parameters\n",
    "    else {}\n",
    ")\n",
    "\n",
    "plt.step(\n",
    "    RECALL,\n",
    "    PRECISION,\n",
    "    where=\"post\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    RECALL, PRECISION, alpha=0.5, **step_kwargs\n",
    ")\n",
    "plt.axhline(\n",
    "    y=AVG_PRECISION,\n",
    "    label=f\"Avg.: {AVG_PRECISION:.3f}\",\n",
    "    linestyle=\"--\",\n",
    "    color=(0.8666666666666667, 0.5176470588235295, 0.3215686274509804),\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.title(f\"Precision-recall (logistic regression)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTSDIR, \"logistic_prc.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "\n",
    "We plot the number of true and false positive predictions, as well as\n",
    "the number of true and false negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(ACTUALS, PREDS)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    confusion / np.sum(confusion),\n",
    "    fmt=\".2%\",\n",
    "    annot=True,\n",
    "    cmap=sns.diverging_palette(230, 20, as_cmap=True),\n",
    "    xticklabels=[\"negative\", \"positive\"],\n",
    "    yticklabels=[\"negative\", \"positive\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted sentiment\")\n",
    "plt.ylabel(\"True sentiment\")\n",
    "plt.title(\"Confusion matrix (logistic regression)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(RESULTSDIR, \"logistic_confuse.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(ACTUALS, PREDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
