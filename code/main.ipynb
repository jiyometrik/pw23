{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting sentiments of hotel reviews through machine learning\n",
    "\n",
    "Welcome to my (admittedly very basic) machine learning project! Here we'll \n",
    "collect our own dataset of hotel reviews, then analyse the dataset, and lastly\n",
    "build machine learning models to predict hotels' sentiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Before we begin, let's settle all our imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "\n",
    "import glob, os, subprocess\n",
    "import zipfile\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", context=\"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "from funcsigs import signature\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    # auc,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "# Preparation\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    SGDClassifier,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting our dataset\n",
    "\n",
    "Let's start by collecting our dataset of hotel reviews. We're using Datafiniti's\n",
    "[Hotel Reviews](https://www.kaggle.com/datasets/datafiniti/hotel-reviews) dataset\n",
    "from Kaggle.\n",
    "\n",
    "We're starting by analysing each review separately. We **tokenise** each\n",
    "review (split it into individual words) and rank each token's sentiment on a numerical\n",
    "scale using the [VADER](https://github.com/cjhutto/vaderSentiment) lexicon.\n",
    "\n",
    "Before you run this code, make sure you've got the Kaggle commandline API\n",
    "installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle d download datafiniti/hotel-reviews --force\n",
    "# subprocess.run([\"kaggle\", \"d\", \"download\", \"datafiniti/hotel-reviews\", \"--force\"])\n",
    "\n",
    "datadir = os.path.join(\n",
    "    os.path.abspath(os.path.dirname(\"\")),\n",
    "    \"../data/\",\n",
    ")\n",
    "\n",
    "resultsdir = os.path.join(\n",
    "    os.path.abspath(os.path.dirname(\"\")),\n",
    "    \"../results/\",\n",
    ")\n",
    "\n",
    "# with zipfile.ZipFile('hotel-reviews.zip', 'r') as zipdata:\n",
    "#     zipdata.extractall(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(datadir, \"7282_1.csv\"))\n",
    "\n",
    "# Remove columns that aren't necessary\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"address\",\n",
    "        \"categories\",\n",
    "        \"city\",\n",
    "        \"country\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"name\",\n",
    "        \"postalCode\",\n",
    "        \"province\",\n",
    "        \"reviews.date\",\n",
    "        \"reviews.dateAdded\",\n",
    "        \"reviews.doRecommend\",\n",
    "        \"reviews.id\",\n",
    "        \"reviews.userCity\",\n",
    "        \"reviews.username\",\n",
    "        \"reviews.userProvince\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Remove rows with NaN\n",
    "df = df.dropna()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the reviews at indices `7`, `8` and `9` aren't in English. We can\n",
    "use `langdetect` to help us detect the language of the review, and then remove the \n",
    "ones that aren't in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"reviews.all\"] = df[\"reviews.text\"] + \" \" + df[\"reviews.title\"]\n",
    "df[\"reviews.language\"] = df[\"reviews.all\"].apply(detect_lang)\n",
    "df = df[df[\"reviews.language\"] == \"en\"]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon-based sentiment analysis\n",
    "\n",
    "We can start using our lexicon to get a numerical value for each of the reviews'\n",
    "sentiments. Let's create an instance of the VADER lexicon analyser and run it through\n",
    "each and every review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df[\"reviews.score\"] = df[\"reviews.all\"].apply(\n",
    "    lambda review: analyzer.polarity_scores(review)[\"compound\"]\n",
    ")\n",
    "\n",
    "# Remove reviews with a score of 0, because they aren't in English\n",
    "df = df[df[\"reviews.score\"] != 0.0]\n",
    "# Remove reviews that are on a scale of 1-10, for standardisation purposes\n",
    "df = df[df[\"reviews.rating\"] <= 5.0]\n",
    "\n",
    "# Classify number of stars review gets into three categories: 1 (positive), 0 (neutral), -1 (negative)\n",
    "max_rating = df[\"reviews.rating\"].max()\n",
    "mid_rating = np.round(\n",
    "    0.5 * max_rating\n",
    ")  # Benchmark to classify positive/neutral/negative\n",
    "df[\"reviews.polarity\"] = df[\"reviews.rating\"].apply(\n",
    "    lambda rating: np.sign(rating - mid_rating)\n",
    ")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how many positive and negative reviews we have, as rated by\n",
    "the lexicon we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_counts = df[\"reviews.polarity\"].value_counts()\n",
    "sns.barplot(x=x_counts.index, y=x_counts).set_title(\n",
    "    \"Proportion of reviews by sentiment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now let's **tokenize** (split our reviews into words) and **lemmatize**\n",
    "(get the root forms of words) our reviews, so that each review becomes\n",
    "a string of lowercase words in their root forms.\n",
    "\n",
    "This should make it much easier to turn them into numbers later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect words we want to exclude\n",
    "nltk.download(\"stopwords\")\n",
    "ickwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def cleanup(review: str) -> str:\n",
    "    tokens = re.findall(r\"[\\w']+\", review)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [\n",
    "        token for token in tokens if token not in ickwords and token.isalpha()\n",
    "    ]\n",
    "    # Lemmatize\n",
    "    tokens = [porter.stem(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"reviews.clean\"] = df[\"reviews.all\"].apply(cleanup)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing tokens\n",
    "\n",
    "Let's take a look at our tokens by way of word clouds: we'll examine the most\n",
    "prominent lemmas that appear in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"crest\", as_cmap=True)\n",
    "\n",
    "lemmas = \" \".join(df[\"reviews.clean\"].values)\n",
    "# print(type(lemmas))\n",
    "cloud = wordcloud.WordCloud(\n",
    "    # font_path=\"clear sans\",\n",
    "    background_color=\"white\",\n",
    "    colormap=\"crest\",\n",
    "    width=1280,\n",
    "    height=960,\n",
    "    collocations=False,\n",
    ")\n",
    "cloud.generate(lemmas)\n",
    "plt.imshow(cloud, interpolation=\"bilinear\")\n",
    "plt.title(\"Word cloud (all lemmas)\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(os.path.join(resultsdir, \"wordcloud.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Let's set our independent and dependent variables: the cleaned review\n",
    "and the review's polarity respectively.\n",
    "\n",
    "Now let's extract numerical representations for each of our reviews. We'll\n",
    "use TF-IDF values (term frequency-inverse document frequency values) to measure\n",
    "how frequently a token appears in a review, relative to other tokens in the same\n",
    "review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"reviews.clean\"]\n",
    "y = df[\"reviews.polarity\"]\n",
    "\n",
    "x.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's vectorize our reviews: turn them into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "The time is finally right to begin training our models! Let's first split\n",
    "the data we have into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=69, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights\n",
    "\n",
    "As you've probably seen before, the number of positive reviews far exceeds the\n",
    "number of negative reviews. If we trained our model with our current data,\n",
    "we might end up with a biased model: one that is more biased towards predicting \n",
    "the majority class (positive). \n",
    "\n",
    "Thus, we'll assign a weight to each class: a higher weight for the minority class,\n",
    "and a lower weight for the majority class. To see which weights are best,\n",
    "let's construct a classic logistic regression model to find our best weights.\n",
    "\n",
    "To do this we'll make a large collection of possible weights our majority class\n",
    "could take, from $0$ to $1$. Then we'll use grid searching to find the class\n",
    "weight that produces the best F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"newton-cg\", class_weight=\"balanced\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "potential_weights = np.linspace(0.0, 0.999, 1000)\n",
    "param_grid = {\n",
    "    \"class_weight\": [\n",
    "        {0: weight, 1: 1.0 - weight} for weight in potential_weights\n",
    "    ]\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedKFold(),\n",
    "    n_jobs=-1,\n",
    "    scoring=\"f1\",\n",
    "    verbose=2,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "weight_tests = pd.DataFrame(\n",
    "    {\n",
    "        \"f1\": gridsearch.cv_results_[\"mean_test_score\"],\n",
    "        \"majority_weight\": (1 - potential_weights),\n",
    "    }\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=weight_tests[\"majority_weight\"],\n",
    "    y=weight_tests[\"f1\"],\n",
    ").set_title(\n",
    "    \"F1 score for logistic regression model against majority class weight\"\n",
    ")\n",
    "plt.savefig(os.path.join(resultsdir, \"logistic_f1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the weight that helps us get the highest F1 score.\n",
    "Let's find the maximum with a bit of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_max = weight_tests[\"f1\"].max()\n",
    "best_weight = weight_tests[weight_tests[\"f1\"] == f1_max][\n",
    "    \"majority_weight\"\n",
    "].values[0]\n",
    "\n",
    "best_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0: 1.0 - best_weight, 1: best_weight}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confused(truths, predictions, label: str, ax=None):\n",
    "    \"\"\"Create a confusion matrix for the given truths and predictions.\"\"\"\n",
    "    confusion = confusion_matrix(truths, predictions)\n",
    "    sns.heatmap(\n",
    "        confusion / np.sum(confusion),\n",
    "        fmt=\".1%\",\n",
    "        annot=True,\n",
    "        cmap=palette,\n",
    "        cbar=False,\n",
    "        xticklabels=[\"negative\", \"positive\"],\n",
    "        yticklabels=[\"negative\", \"positive\"],\n",
    "        ax=ax,\n",
    "    ).set_title(f\"Confusion matrix ({label})\")\n",
    "    if ax is not None:\n",
    "        ax.set_xlabel(\"Predicted sentiment\")\n",
    "        ax.set_ylabel(\"True sentiment\")\n",
    "    else:\n",
    "        plt.xlabel(\"Predicted sentiment\")\n",
    "        plt.ylabel(\"Predicted sentiment\")\n",
    "    return confusion\n",
    "\n",
    "\n",
    "lr_preds = lr.predict(X_test)\n",
    "confused(y_test, lr_preds, \"logistic regression\")\n",
    "plt.savefig(os.path.join(resultsdir, \"logistic_confuse.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(truths, predictions, label: str, ax):\n",
    "    \"\"\"Create a precision-recall curve for the given truths and predictions.\"\"\"\n",
    "    avg_precision = average_precision_score(truths, predictions, pos_label=1)\n",
    "    precision, recall, _ = precision_recall_curve(truths, predictions)\n",
    "    step_kwargs = (\n",
    "        {\"step\": \"post\"}\n",
    "        if \"step\" in signature(plt.fill_between).parameters\n",
    "        else {}\n",
    "    )\n",
    "    ax.step(\n",
    "        recall,\n",
    "        precision,\n",
    "        where=\"post\",\n",
    "    )\n",
    "    ax.fill_between(recall, precision, alpha=0.5, **step_kwargs)\n",
    "    ax.axhline(\n",
    "        y=avg_precision,\n",
    "        label=f\"Avg.: {avg_precision:.3f}\",\n",
    "        linestyle=\"--\",\n",
    "        color=(0.8666666666666667, 0.5176470588235295, 0.3215686274509804),\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_title(f\"Precision-recall curve ({label})\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(truths, predictions, label: str, ax):\n",
    "    \"\"\"Create a ROC curve for the given truths and predictions.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(truths, predictions)\n",
    "    roc_auc = roc_auc_score(truths, predictions)\n",
    "    ax.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=(0.8666666666666667, 0.5176470588235295, 0.3215686274509804),\n",
    "        label=f\"Area: {roc_auc:.2f}\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        linestyle=\"--\",\n",
    "        label=\"Random classifier\",\n",
    "    )\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.0])\n",
    "    ax.set_xlabel(\"False positive rate\")\n",
    "    ax.set_ylabel(\"True positive rate\")\n",
    "    ax.set_title(f\"ROC curve ({label})\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "\n",
    "Let's first create a random forest classifier, a classification model \n",
    "commonly used in text classification, like this. It's quite popular because\n",
    "it yields high accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, class_weight=weights)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate it on a few metrics. First, let's see a report on how it did\n",
    "classifying our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite good! We have an F1 score of $0.94$ for the positive reviews, and high\n",
    "precisions for both classes. Now let's see a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_confusion = confusion_matrix(y_test, rf_preds)\n",
    "rf_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confused(y_test, rf_preds, \"random forest\")\n",
    "plt.savefig(os.path.join(resultsdir, \"rndforst_confuse.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine (SVM)\n",
    "\n",
    "Let's try a support vector machine next! They're also quite widely used within\n",
    "the realm of text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss=\"hinge\", class_weight=weights)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_preds = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again test it with our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confused(y_test, svm_preds, \"support vector machine\")\n",
    "plt.savefig(os.path.join(resultsdir, \"svm_confuse.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "\n",
    "The Naive Bayes classifier is a probabilistic machine learning model based on Bayes' theorem.\n",
    "It's not that well-known, but has been shown to produce decent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = BernoulliNB()\n",
    "naive.fit(np.asarray(X_train.todense()), y_train, sample_weight=weights[0])\n",
    "naive_preds = naive.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know the drill. Let's make a report and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, naive_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confused(y_test, naive_preds, \"Naive Bayes\")\n",
    "plt.savefig(os.path.join(resultsdir, \"nb_confuse.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbours\n",
    "\n",
    "Lastly, let's round this classification exercise off with a K-nearest neighbours\n",
    "(KNN) classifier. It's most commonly used in any type of classification problem,\n",
    "so it's definitely worth a shot.\n",
    "\n",
    "KNN doesn't use numerical weights as we've been doing for other classification models,\n",
    "but we can weigh points based on the inverse of their distance. Let's use that then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\n",
    "knn.fit(X_train, y_train)\n",
    "knn_preds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's use the same metrics: a report and a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, knn_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confused(y_test, knn_preds, \"K-nearest neighbours\")\n",
    "plt.savefig(os.path.join(resultsdir, \"knn_confuse.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "\n",
    "We'll now use an **unsupervised classification** technique to classify the reviews.\n",
    "K-means clustering is one of the more popular choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, init=\"random\")\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "kmeans_preds = kmeans.predict(X_test)\n",
    "confused(y_test, kmeans_preds, \"K-means clustering\")\n",
    "plt.savefig(os.path.join(resultsdir, \"kmeans_confuse.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidating our metrics\n",
    "\n",
    "Finally, let's get as much metrics as we want from all our models, and then export\n",
    "them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    lr: lr.predict(X_test),\n",
    "    rf: rf_preds,\n",
    "    svm: svm_preds,\n",
    "    knn: knn_preds,\n",
    "    naive: naive_preds,\n",
    "    kmeans: kmeans_preds,\n",
    "}\n",
    "approximate = lambda x: round(x, 3)\n",
    "model_metrics = pd.DataFrame(\n",
    "    data={\n",
    "        \"name\": [model.__class__.__name__ for model, _ in models.items()],\n",
    "        \"f1\": [\n",
    "            approximate(f1_score(y_test, preds)) for _, preds in models.items()\n",
    "        ],\n",
    "        \"auroc\": [\n",
    "            approximate(roc_auc_score(y_test, preds))\n",
    "            for _, preds in models.items()\n",
    "        ],\n",
    "        \"averageprecision\": [\n",
    "            approximate(average_precision_score(y_test, preds))\n",
    "            for _, preds in models.items()\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for i, (model, preds) in enumerate(models.items()):\n",
    "    precision_recall(\n",
    "        y_test, preds, model.__class__.__name__, axes[i // 3, i % 3]\n",
    "    )\n",
    "f.suptitle(f\"Precision-recall curves\")\n",
    "plt.savefig(os.path.join(resultsdir, f\"all_prc.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for i, (model, preds) in enumerate(models.items()):\n",
    "    confused(y_test, preds, model.__class__.__name__, axes[i // 3, i % 3])\n",
    "f.suptitle(f\"Confusion matrices\")\n",
    "plt.savefig(os.path.join(resultsdir, f\"all_confuse.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for i, (model, preds) in enumerate(models.items()):\n",
    "    roc(y_test, preds, model.__class__.__name__, axes[i // 3, i % 3])\n",
    "f.suptitle(f\"Receiver operating characteristic curves\")\n",
    "plt.savefig(os.path.join(resultsdir, f\"all_roc.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old model metrics\n",
    "for f in glob.glob(os.path.join(resultsdir, \"model_metrics_*\")):\n",
    "    os.remove(f)\n",
    "\n",
    "# Save new model metrics\n",
    "today = pd.to_datetime(\"today\").strftime(\"%d%m%Y\")\n",
    "model_metrics.to_csv(\n",
    "    os.path.join(\n",
    "        resultsdir,\n",
    "        f\"model_metrics_{today}.csv\",\n",
    "    ),\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
