{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question 3: Random Forest Classifier\n",
    "\n",
    "Here, we build a simple random forest classifier to classify hotel reviews\n",
    "into two basic categories: those with negative and those with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "\n",
    "# fundamentals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"talk\", style=\"white\")\n",
    "\n",
    "# natural language processing\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# turn a document to a vector\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# machine learning imports\n",
    "from funcsigs import signature\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "DATADIR = os.path.join(\n",
    "    os.path.abspath(os.path.dirname(\"\")), \"../data\"\n",
    ")\n",
    "DF = pd.read_csv(\n",
    "    os.path.join(DATADIR, \"combined_sentiments.csv\"),\n",
    "    header=0,\n",
    "    sep=\",\",\n",
    "    on_bad_lines=\"skip\",\n",
    ")\n",
    "STOP = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "# lemmatise\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"lemmatises words by classifying them into their\n",
    "    respective parts of speech.\"\"\"\n",
    "    if pos_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "\n",
    "def check_digits(text):\n",
    "    \"\"\"is there a digit in the text?\"\"\"\n",
    "    return any(i.isdigit() for i in text)\n",
    "\n",
    "\n",
    "def clean_review(review):\n",
    "    \"\"\"tokenise and clean up punctuation\"\"\"\n",
    "    review = str(review).lower()\n",
    "    review = [\n",
    "        word.strip(string.punctuation)\n",
    "        for word in review.split(\" \")\n",
    "    ]  # remove punctuation\n",
    "    review = [\n",
    "        word for word in review if not check_digits(word)\n",
    "    ]  # remove digits\n",
    "\n",
    "    # remove stop words\n",
    "    review = [\n",
    "        token for token in review if token not in STOP\n",
    "    ]\n",
    "\n",
    "    # remove empty tokens\n",
    "    review = [token for token in review if len(token) > 0]\n",
    "\n",
    "    # tag each token with its part of speech (pos)\n",
    "    pos_tags = pos_tag(review)\n",
    "    review = [\n",
    "        WordNetLemmatizer().lemmatize(\n",
    "            tag[0], get_wordnet_pos(tag[1])\n",
    "        )\n",
    "        for tag in pos_tags\n",
    "    ]\n",
    "\n",
    "    # remove words with only one letter\n",
    "    review = [token for token in review if len(token) > 1]\n",
    "    review = \" \".join(review)\n",
    "    return review\n",
    "\n",
    "\n",
    "# generate a cleaned, tokenised and lemmatised version of the reviews\n",
    "DF[\"reviews.clean\"] = DF[\"reviews.text\"].apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vector representations for each review\n",
    "DOCUMENTS = [\n",
    "    TaggedDocument(doc, [i])\n",
    "    for i, doc in enumerate(\n",
    "        DF[\"reviews.clean\"].apply(lambda x: x.split(\" \"))\n",
    "    )\n",
    "]\n",
    "\n",
    "# train a doc2vec model\n",
    "MODEL = Doc2Vec(\n",
    "    DOCUMENTS,\n",
    "    vector_size=5,\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    ")\n",
    "\n",
    "# transform each document into vec data\n",
    "DF_VEC = (\n",
    "    DF[\"reviews.clean\"]\n",
    "    .apply(lambda x: MODEL.infer_vector(x.split(\" \")))\n",
    "    .apply(pd.Series)\n",
    ")\n",
    "DF_VEC.columns = [\"vec_\" + str(x) for x in DF_VEC.columns]\n",
    "DF = pd.concat([DF, DF_VEC], axis=1)\n",
    "# DF_VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "We can calculate the TF-IDF values of each review. This gives\n",
    "us a list of values, denoting how often any token appears. We can use\n",
    "these values as weights to determine how important a token is in determining\n",
    "a review's overall sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TfidfVectorizer(min_df=10)\n",
    "tfidf_result = TFIDF.fit_transform(\n",
    "    DF[\"reviews.clean\"]\n",
    ").toarray()\n",
    "DF_TFIDF = pd.DataFrame(\n",
    "    tfidf_result, columns=TFIDF.get_feature_names_out()\n",
    ")\n",
    "DF_TFIDF.columns = [\n",
    "    \"word_\" + str(x) for x in DF_TFIDF.columns\n",
    "]\n",
    "DF_TFIDF.index = DF.index\n",
    "DF = pd.concat([DF, DF_TFIDF], axis=1)\n",
    "DF_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution\n",
    "\n",
    "We can also examine how concentrated tokens of a specific sentiment score are by\n",
    "plotting a simple histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTSDIR = os.path.join(\n",
    "    os.path.abspath(os.path.dirname(\"\")),\n",
    "    \"../results/\",\n",
    ")\n",
    "plt.title(\"Distribution of reviews by sentiment score\")\n",
    "plt.xlabel(\"Sentiment score\")\n",
    "plt.ylabel(\"Number of reviews\")\n",
    "\n",
    "SUBSET_NEG, SUBSET_NEU, SUBSET_POS = (\n",
    "    DF[DF[\"sent.polarity\"] == i] for i in list(range(-1, 2))\n",
    ")\n",
    "plt.hist(SUBSET_NEG[\"sent.net\"], label=\"negative\")\n",
    "plt.hist(SUBSET_NEU[\"sent.net\"], label=\"neutral\")\n",
    "plt.hist(SUBSET_POS[\"sent.net\"], label=\"positive\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTSDIR, \"distribution.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "We can then select features we want to train our model on; we select two for\n",
    "the independent and dependent variables respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF[\"review.is_neg\"] = DF[\"sent.polarity\"].apply(\n",
    "    lambda x: x == -1\n",
    ")\n",
    "LABEL = \"review.is_neg\"\n",
    "COLS_TO_IGNORE = [\n",
    "    LABEL,\n",
    "    \"sent.polarity\",\n",
    "    \"sent.pos\",\n",
    "    \"sent.neg\",\n",
    "    \"sent.net\",\n",
    "    \"index\",\n",
    "    \"reviews.rating\",\n",
    "    \"reviews.clean\",\n",
    "    \"reviews.title\",\n",
    "    \"reviews.text\",\n",
    "]\n",
    "FEATURES = [\n",
    "    col for col in DF.columns if col not in COLS_TO_IGNORE\n",
    "]\n",
    "\n",
    "\"\"\"split the data into training and testing sets.\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DF[FEATURES], DF[LABEL], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Meat\n",
    "\n",
    "We finally can train our random forest classifier! Let's also examine\n",
    "the feature importance the model has evaluated for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"random forest classifier!\"\"\"\n",
    "\n",
    "RF = RandomForestClassifier(\n",
    "    n_estimators=100, random_state=42\n",
    ")\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"feature importance\"\"\"\n",
    "FEATURE_IMPORTANCES = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": FEATURES,\n",
    "        \"importance\": RF.feature_importances_,\n",
    "    }\n",
    ").sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "Let's get the model to make precisions. After that, we can plot the\n",
    "**precision-recall** and **receiver operating characteristic** curves\n",
    "to see how accurate the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [pred[1] for pred in RF.predict_proba(X_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic\n",
    "\n",
    "We can compare the predictions with the original dataset to evaluate the\n",
    "**true positive rate** and **false positive rate**. After that, we can\n",
    "plot the true positive rate against the false positive rate to obtain a receiver\n",
    "operating characteristic curve. The higher the curve lies over the dotted line\n",
    "(where the true and false positive rates are equal), the more accurate\n",
    "the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FALSE_POS_RATE, TRUE_POS_RATE, thresholds = roc_curve(\n",
    "    y_test, y_pred, pos_label=1\n",
    ")\n",
    "roc_auc = auc(FALSE_POS_RATE, TRUE_POS_RATE)\n",
    "\n",
    "plt.plot(\n",
    "    FALSE_POS_RATE,\n",
    "    TRUE_POS_RATE,\n",
    "    label=f\"ROC (Area: {roc_auc:.3f})\",\n",
    ")\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    linestyle=\"--\",\n",
    "    label=\"Random classifier\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.title(\"ROC (random forest)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(os.path.join(RESULTSDIR, \"rndforst_roc.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall \n",
    "\n",
    "write some stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_PRECISION = average_precision_score(\n",
    "    y_test, y_pred, pos_label=1\n",
    ")\n",
    "PRECISION, RECALL, _ = precision_recall_curve(\n",
    "    y_test, y_pred\n",
    ")\n",
    "step_kwargs = (\n",
    "    {\"step\": \"post\"}\n",
    "    if \"step\" in signature(plt.fill_between).parameters\n",
    "    else {}\n",
    ")\n",
    "\n",
    "plt.step(\n",
    "    RECALL,\n",
    "    PRECISION,\n",
    "    where=\"post\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    RECALL, PRECISION, alpha=0.5, **step_kwargs\n",
    ")\n",
    "plt.axhline(\n",
    "    y=AVG_PRECISION,\n",
    "    label=f\"Avg.: {AVG_PRECISION:.3f}\",\n",
    "    linestyle=\"--\",\n",
    "    color=(0.8666666666666667, 0.5176470588235295, 0.3215686274509804),\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.title(\"Precision-recall (random forest)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(os.path.join(RESULTSDIR, \"rndforst_prc.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "\n",
    "We plot the number of true and false positive predictions, as well as\n",
    "the number of true and false negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bin = np.where(\n",
    "    np.array(y_pred) > np.median(y_pred), True, False\n",
    ")\n",
    "confusion = confusion_matrix(y_test, y_pred_bin)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    confusion / np.sum(confusion),\n",
    "    fmt=\".2%\",\n",
    "    annot=True,\n",
    "    cmap=sns.diverging_palette(230, 20, as_cmap=True),\n",
    "    xticklabels=[\"negative\", \"positive\"],\n",
    "    yticklabels=[\"negative\", \"positive\"],\n",
    ")\n",
    "plt.title(\"Confusion matrix (random forest)\")\n",
    "plt.xlabel(\"Predicted sentiment\")\n",
    "plt.ylabel(\"True sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(RESULTSDIR, \"rndforst_confuse.png\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
